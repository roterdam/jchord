\Section{experiments}{Experiments}

In this section, we apply our \PR\ algorithm (\refsec{algorithm}) to
$k$-object-sensitivity for three clients:
downcast safety checking (\downcast),
monomorphic call site inference (\monosite),
and race detection (\race).  These analyses are described in \refsec{klimited}.
Our main empirical result is that across different clients and benchmarks,
pruning is very effective at curbing the exponential growth,
which allows us to run the analysis on very refined abstractions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{setup}{Setup}

% Benchmark/setup
Our experiments were performed using IBM J9VM 1.6.0 on 64-bit Linux machines.
All analyses were implemented in Chord,
an extensible program analysis framework for Java bytecode,\footnote{{\tt{http://code.google.com/p/jchord/}}}
which uses the BDD solver {\tt bddbddb} \cite{Whaley2007}.
We evaluated on \numBenchmarks\ Java benchmarks shown in \reftab{benchmarks}.
In each run, we allocated 8GB of memory and terminated the process when it ran out of memory.

\begin{table*}
\centering
\input figures/stats.tex
\caption{Benchmark characteristics:
the number of classes,
number of methods,
total number of bytecodes in these methods,
and number of allocation sites ($|\H|$)
deemed reachable by 0-CFA.
\label{tab:benchmarks}}
\end{table*}

% Algorithms
We experimented with various combinations of abstractions and refinement
algorithms (see \reftab{algorithms}).  As a baseline, we consider running an analysis
with a full abstraction $\alpha$ (denoted $\Full(\alpha)$).
For $\alpha$, we can either use $k$-limited abstractions
($(\klimabs_k)_{k=1}^\infty$), in which case we recover ordinary $k$-object-sensitivity,
or the barely-repeating variants ($(\klimdabs_k)_{k=1}^\infty$).
We also consider the site-based refinement algorithm of \cite{liang11minimal},
which considers a sequence of abstractions $\balpha = (\balpha_0, \balpha_1, \dots)$
but stops refining sites which have been deemed irrelevant (this algorithm is denoted $\Site(\balpha)$).

As for the new methods that we propose in this paper,
we have $\PR(\balpha)$, which uses a sequence of abstractions $\balpha$, which does no pre-pruning,
or $\PR(\balpha,\hclass)$, which performs pre-pruning using $\beta_t = \alpha_t
\circ \hclass$ for $t = 0, 1, 2, \dots$.  We consider three choices of $\hclass$
($\hclassIs,\hclassHas,\hclassIsHas$) which use different kinds of type information.

In our implementation of the \PR\ algorithm, we depart slightly from our
presentation.  Instead of maintaining the set of relevant input tuples as
determined by our $k$-limited Datalog program across iterations, we instead
maintain the allocation site chains which were involved in any relevant input
tuple.  We can modify the original Datalog program so that the \PR\ algorithm
is consistent with this implementation by introducing a new input relation
$\dl{active}(c)$ for $c \in \H^*$, encoding existing input relations as rules
consisting of zero source terms, and adding $\dl{active}(c)$ for each existing
rule that uses a variable $c$ which is an allocation site.  Now computing the
relevant input tuples corresponds exactly to computing the set of relevant
allocation site chains.

\begin{table}
\[
\begin{array}{ll}
\text{\bf Abstractions} \\
\Klimabs = (\klimabs_k)_{k=1}^\infty   & \defn{$k$-limited abstractions \refeqn{klimitedAbstraction}} \\
\Klimdabs = (\klimdabs_k)_{k=1}^\infty & \defn{barely-repeating $k$-limited abstractions \refeqn{barelyAbstraction}} \\
\hclassIs                              & \defn{abstraction using type of allocation site} \\
\hclassHas                             & \defn{abstraction using type of containing class} \\
\hclassIsHas                           & \defn{abstraction using both types} \\
\\
\text{\bf Algorithms} \\
\Full(\alpha)         & \defn{standard analysis using abstraction $\alpha$} \\
\Site(\balpha)        & \defn{site-based refinement \cite{liang11minimal} on abstractions $\balpha$} \\
\PR(\balpha)          & \defn{\PR\ algorithm using $\balpha$, no pre-pruning} \\
\PR(\balpha,\hclass)  & \defn{\PR\ algorithm using $\balpha$, using $\balpha\circ\hclass$ to pre-prune} \\
\end{array}
\]
\caption{\label{tab:algorithms} Shows the abstractions and algorithms that we evaluated empirically.
For example,
$\PR(\Klimdabs,\hclassIsHas)$
means running the \PR-algorithm on the barely-repeating $k$-limited abstraction ($\alpha_k = \klimdabs_k$),
using a composed abstraction based on the type of an allocation site ($\hclassIs$) 
and the type of the declaring class ($\hclassHas$) to do pre-pruning
(specifically, $\beta_k = \klimdabs_k \circ \hclassIsHas$).
}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{results}{Results}

\reffig{absSizeGraph} compares the number of input tuples needed by various
algorithms; we see that the non-pruning methods completely hit a wall, whereas
the pruning methods are able to continue increasing $k$ farther.  We also
observed that the running time of these analyses tracked the number of tuples
somewhat, so we do obtain speedups, but the differences are less pronounced.
One reason is that we are using BDDs, which can handle large numbers of tuples
so long as they are structured; pruning destroys some of this structure,
yielding less predictable running times.

\FigStar{figures/absSizeGraph}{0.35}{absSizeGraph}{For each client/benchmark pair,
we show the growth of the number of input tuples $|A_t'|$ across iterations.
Recall that $|A_t'|$ is the number of tuples fed into the Datalog program.
\reftab{algorithms} describes the four methods.
We see that the methods that prune drastically cut down the number of input tuples
by many orders of magnitude.
}

\reftab{sizeRatio} provides more on the quantitative impact of pruning in our
best instantiation of the \PR\ algorithm.  We see that pre-pruning has a
significant positive impact: we are able to perform pruning at a much cheaper
level (using types instead of allocation sites).  Many of the tuples are pruned
at this level, and the results of this pruning carry over to the original
$k$-limited abstraction.

\begin{table}
\centering
\small
\input figures/sizeRatio
\caption{\label{tab:sizeRatio} Shows the shrinking and growth of the
number of tuples during the various pruning and refinement operations (see \reffig{pseudocode}) for 
our best method $\PR(\Klimabs,\hclassIsHas)$ across all the clients and benchmarks, averaged across iterations.
The columns are as follows:
First, $\frac{|B_t|}{|A_t|}$ measures the number of tuples after projecting
down to the auxiliary abstraction $\beta_t$ for pre-pruning; note that running the analysis
using types instead of allocation sites is much cheaper.
Next, $\frac{|\tilde B_t|}{|B_t|}$ shows the fraction of abstract values kept during pre-pruning;
When we return from types to allocation sites, see that the effect of pre-pruning essentially carries over
($\frac{|A_t'|}{|A_t|}$).
Recall that we do the pruning step after pre-pruning;
$\frac{|\tilde A_t|}{|A_t|}$ measures the total fraction of chains kept during pruning.
Finally, $\frac{|A_{t+1}|}{|A_t|}$ measures the ratio between iterations,
which includes both pruning and refinement.  Note that there is still almost a three-fold
growth of the number of tuples (on average), but this growth would have been much more unmanageable
without the pruning.
}
\end{table}

So far, we have been using the $k$-limited abstraction.  In
\reffig{barelyRepeatingEffect}, we show that when we can increase the $k$
value, the barely-repeating $k$-limited abstraction is helpful, but in most
cases, it does not improve scalability.  The reason is that the
barely-repeating abstraction curbs refinement, but often, and quite
paradoxically, it is exactly refinement which enables us to prune more.

\FigStar{figures/barelyRepeatingEffect}{0.35}{barelyRepeatingEffect}{
Shows the 5 (out of 15) client/benchmark pairs for which using the
barely-repeating $k$-limited abstraction ($\Klimdabs$) allows one to increase
$k$ much more than the plain $k$-limited abstraction ($\Klimabs$).
On the other 10 client/benchmarks where $k$ cannot get very large,
limiting repetitions is actually slightly worse in terms of scalability.
}

Finally, \reftab{numUnproven} shows the effect on the number of queries proven.
While pruning enables us to increase $k$ much more than before, it turns out
that our particular analyses for these clients saturate quite quickly, so we
are only able to prove two more queries than using the non-pruning techniques.
On the surface, these findings seem to contradict \cite{liang10abstraction},
which showed a sharp increase in precision around $k=4$ for $k$-CFA.  However,
this discrepancy merely suggests that our flow-insensitive analyses are simply
limited: since \cite{liang10abstraction} obtains a lower bound, we know for
sure that low $k$ values are insufficient; the fact that we don't see an
increase in precision suggests that the non-$k$-related aspects of our analyses
are insufficient.  However, given that our pruning approach is general, it
would be interesting tackling other aspects such as flow-sensitivity.

\begin{table}
\small
\centering
\input figures/numUnproven
\caption{\label{tab:numUnproven} The number of unproven queries (unsafe
downcasts, polymorphic sites, races) for each of the clients and benchmarks
over the first five iterations.
All analyses obtain the exact results on iterations where they obtain an answer.
Bolded numbers are ones obtained by our best method $\PR(\Klimabs,\hclassIsHas)$
but not by the full algorithm.
We see that although pruning enables to increase $k$ by much more than before,
only for two the client/benchmark pairs do we get strictly more precise results.
This points out inherent limitations of this family of $k$-limited abstractions.
}
\end{table}
