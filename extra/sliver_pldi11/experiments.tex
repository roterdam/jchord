\Section{experiments}{Experiments}

In this section, we apply the Prune-Refine algorithm (\refsec{algorithm}) to
$k$-object-sensitivity for our three clients (\refsec{clients}):
downcast safety checking (\downcast),
monomorphic call site inference (\monosite),
and race detection (\race).
Our main empirical result is that across different clients and benchmarks,
pruning is effective at curbing the exponential growth,
which allows us to run analyses using abstractions finer than what is possible without pruning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{setup}{Setup}

% Benchmark/setup
Our experiments were performed using IBM J9VM 1.6.0 on 64-bit Linux machines.
All analyses were implemented in Chord,
an extensible program analysis framework for Java bytecode,\footnote{{\tt{http://code.google.com/p/jchord/}}}
which uses the BDD Datalog solver {\tt bddbddb} \cite{Whaley2007}.
We evaluated our analyses on \numBenchmarks\ Java benchmarks shown in \reftab{benchmarks}.
In each run, we allocated 8GB of memory and terminated the process when it ran out of memory.

\begin{table*}
\centering
\input figures/stats.tex
\caption{Benchmark characteristics:
the number of classes,
number of methods,
total number of bytecodes in these methods,
and number of allocation sites ($|\H|$)
deemed reachable by 0-CFA.
\label{tab:benchmarks}}
\end{table*}

% Algorithms
We experimented with various combinations of abstractions and refinement
algorithms (see \reftab{algorithms}).  As a baseline, we consider running an analysis
with a full abstraction $\alpha$ (denoted $\Full(\alpha)$).
For $\alpha$, we can either use $k$-limited abstractions
($\Klimabs = (\klimabs_k)_{k=0}^\infty$), in which case we recover ordinary $k$-object-sensitivity,
or the barely-repeating variants ($\Klimdabs = (\klimdabs_k)_{k=0}^\infty$).
We also consider the site-based refinement algorithm of \cite{liang11minimal},
which considers a sequence of abstractions $\balpha = (\alpha_0, \alpha_1, \dots)$
but stops refining sites which have been deemed irrelevant.  This algorithm is denoted $\Site(\balpha)$.

As for the new algorithms that we propose in this paper,
we have $\PR(\balpha)$, which corresponds to the Prune-Refine (\PR) algorithm using a sequence of abstractions $\balpha$ with no pre-pruning;
and $\PR(\balpha,\hclass)$, which performs pre-pruning using $\beta_t = \alpha_t
\circ \hclass$ for $t = 0, 1, 2, \dots$.  We consider three choices of $\hclass$
which use different kinds of type information ($\hclassIs,\hclassHas,\hclassIsHas$).

In our implementation of the Prune-Refine algorithm, we depart slightly from our
presentation.  Instead of maintaining the full set of relevant input tuples, we instead
maintain only the set of allocation site chains which exist in some relevant input
tuple.  This choice results in more conservative pruning, but reduces the
amount of information that we have to keep.
We can modify the original Datalog program so that the original Prune-Refine algorithm
computes this new variant: Specifically, first introduce new input tuples
$\dl{active}(c)$ for each $c \in \H^*$.   Then encode existing $\ext$ input tuples as rules
with no source terms; $\ext$ is no longer an input relation.  Finally, add $\dl{active}(c)$ to the right-hand side of each existing
rule that uses a chain-valued variable.  Computing the
relevant input tuples in this modified Datalog program corresponds exactly to
computing the set of relevant allocation site chains.

\begin{table}
\[
\begin{array}{ll}
\text{\bf Abstractions} \\
\Klimabs = (\klimabs_k)_{k=0}^\infty   & \defn{$k$-limited abstractions \refeqn{klimitedAbstraction}} \\
\Klimdabs = (\klimdabs_k)_{k=0}^\infty & \defn{barely-repeating $k$-limited abstractions \refeqn{barelyAbstraction}} \\
\hclassIs                              & \defn{abstraction using type of allocation site} \\
\hclassHas                             & \defn{abstraction using type of containing class} \\
\hclassIsHas                           & \defn{abstraction using both types} \\
\\
\text{\bf Algorithms} \\
\Full(\alpha)         & \defn{standard analysis using an abstraction $\alpha$} \\
\Site(\balpha)        & \defn{site-based refinement \cite{liang11minimal} on abstractions $\balpha$} \\
\PR(\balpha)          & \defn{\PR\ algorithm using $\balpha$, no pre-pruning} \\
\PR(\balpha,\hclass)  & \defn{\PR\ algorithm using $\balpha$, using $\balpha\circ\hclass$ to pre-prune} \\
\end{array}
\]
\caption{\label{tab:algorithms} Shows the abstractions and algorithms that we evaluated empirically.
For example,
$\PR(\Klimdabs,\hclassIsHas)$
means running the Prune-Refine algorithm on the barely-repeating $k$-limited abstraction ($\alpha_k = \klimdabs_k$),
using a composed abstraction based on the type of an allocation site ($\hclassIs$) 
and the type of the declaring class ($\hclassHas$) to do pre-pruning
(specifically, $\beta_k = \klimdabs_k \circ \hclassIsHas$).
}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{results}{Results}

We ran the four algorithms of \reftab{algorithms} using $k$-limited abstractions, seeing
how far we could increase $k$ until the analyses ran out of memory.
For each analysis, we also measured the number of input tuples given to
the Datalog solver; this quantity is denoted as $|A_t'|$ (see
\reffig{pseudocode}).  In this section, the number of iterations $t$ is the same as the $k$ value.

\reffig{absSizeGraph} plots the number of tuples $|A_t'|$ as a function of
number of iterations $t$.  We see that the
non-pruning algorithms completely hit a wall after a few iterations, with the
number of tuples exploding exponentially.  On most benchmark-client pairs, the
pruning algorithms are able to continue increasing $k$ much further, though on several
pairs, pruning only manages to increase $k$ by one beyond the non-pruning algorithms.
We also observed that pruning does yield speedups, although these are
less pronounced than the differences in the number of tuples.  Nonetheless, pruning
overcomes the major bottleneck---that standard $k$-limited analyses run
out of memory even for moderate $k$.  By curbing the growth of the
number of tuples, pruning makes it possible to run some analyses at all.

However, there are several caveats with pruning:
First, we are using BDDs, which can actually handle large numbers of tuples
so long as they are structured; pruning destroys some of this structure,
yielding less predictable running times.  Second, pruning requires solving
the transformed Datalog program for computing $\bP(X)$, which is more expensive than the original Datalog program.
Finally, we must solve the Datalog program several times, not just once.
These three caveats also apply to the site-based refinement algorithm of \cite{liang11minimal} (\Site),
so pruning is at least a strict improvement over that algorithm.

\FigStar{figures/absSizeGraph}{0.35}{absSizeGraph}{For each client/benchmark pair,
we show the growth of the number of input tuples $|A_t'|$ across iterations
(recall that $A_t'$ are the tuples fed into the Datalog program).
\reftab{algorithms} describes the four algorithms.
We see that the pruning algorithms ($\PR(\Klimabs)$ and $\PR(\Klimabs,\hclassIsHas)$) drastically cut down the number of input tuples
by many orders of magnitude.
}

We found that the best instantiation of the Prune-Refine algorithm is $\PR(\Klimabs,
\hclassIsHas)$, which involves pre-pruning with both kinds of type information ($\hclassIsHas$);
this works better than both no pre-pruning and pre-pruning with only
$\hclassIs$ or $\hclassHas$ alone.

\reftab{sizeRatio} provides more details on the quantitative impact of pruning for $\PR(\Klimabs, \hclassIsHas)$.  We see that pre-pruning has a
significant impact: we can eliminate about three-quarters of the tuples by just operating on the coarser
level of types rather than allocation sites (see the $\frac{|\tilde B_t|}{|B_t|}$ column).
Importantly, the effect of this pruning carries over to the original
$k$-limited abstraction (see the $\frac{|A_t'|}{|A_t|}$ column).

\begin{table}
\centering
\small
\input figures/sizeRatio
\caption{\label{tab:sizeRatio} Shows the shrinking and growth of the
number of tuples during the various pruning and refinement operations (see \reffig{pseudocode}) for 
our best algorithm $\PR(\Klimabs,\hclassIsHas)$ across all the clients and benchmarks, averaged across iterations.
The columns are as follows:
First, $\frac{|B_t|}{|A_t|}$ measures the number of tuples after projecting
down to the auxiliary abstraction $\beta_t = \klimabs_t \circ \hclassIsHas$ for pre-pruning; note that running the analysis
using types instead of allocation sites is much cheaper.
Next, $\frac{|\tilde B_t|}{|B_t|}$ shows the fraction of abstract values kept during pre-pruning;
When we return from types to allocation sites, we see that the effect of pre-pruning carries over
($\frac{|A_t'|}{|A_t|}$).
Next, pruning kept $\frac{|\tilde A_t|}{|A_t|}$ of the chains.
Finally, $\frac{|A_{t+1}|}{|A_t|}$ measures the ratio between iterations,
which includes both pruning and refinement.  Note that there is still almost a three-fold
growth of the number of tuples (on average), but this growth would have been much more unmanageable
without the pruning.
}
\end{table}

So far, we have been using the $k$-limited abstraction; we now compare this abstraction
with the barely-repeating $k$-limited abstraction introduced in \refsec{barelyAbstraction}.
As \reffig{barelyRepeatingEffect} shows, for a few cases,
the barely-repeating $k$-limited abstraction requires fewer tuples than the $k$-limited abstraction; but in most
cases, it does not improve scalability.  The reason is that the
barely-repeating abstraction curbs refinement, but often, somewhat
paradoxically, it is exactly the refinement which enables more pruning.

\FigStar{figures/barelyRepeatingEffect}{0.35}{barelyRepeatingEffect}{
Shows the 5 (out of the 15) client/benchmark pairs for which using the
barely-repeating $k$-limited abstraction ($\Klimdabs$) allows one to increase
$k$ much more than the plain $k$-limited abstraction ($\Klimabs$).
On the other 10 client/benchmarks where $k$ cannot get very large,
limiting repetitions is actually slightly worse in terms of scalability.
Also note that three of the plots for $\Klimdabs$ stop early, not because the algorithm runs out of memory,
but because the algorithm has actually converged and increasing $k$ would have no effect.
}

Finally, \reftab{numUnproven} shows the effect on the number of queries proven.
While pruning enables us to increase $k$ much more than before, it turns out
that our particular analyses for these clients saturate quite quickly, so
over all the clients and benchmarks,
we were only able to prove two queries more than using the non-pruning techniques.
On the surface, these findings seem to contradict \cite{liang10abstraction},
which showed a sharp increase in precision around $k=4$ for $k$-CFA.  However,
this discrepancy merely suggests that our flow-insensitive analyses are simply
limited: since \cite{liang10abstraction} offers upper bounds on precision, we know for
sure that low $k$ values are insufficient; the fact that we don't see an
increase in precision for higher $k$ suggests that the non-$k$-related aspects of our analyses
are insufficient.  Given that our pruning approach is general, it
would be interesting to tackle other aspects of program analysis such as
flow-sensitivity.

\begin{table}
\small
\centering
\input figures/numUnproven
\caption{\label{tab:numUnproven} The number of unproven queries (unsafe
downcasts, polymorphic sites, races) for each of the clients and benchmarks
over the first five iterations.
All analyses obtain the exact results on iterations where they obtain an answer.
Bolded numbers refer to $k$ values reached by $\PR(\Klimabs,\hclassIsHas)$
but not by any non-pruning algorithm.
While pruning enables to increase $k$ more, we get strictly more precise results
for only two of the client/benchmark pairs (\downcast/\hedc\ and \downcast/\lusearch).
This points out inherent limitations of this family of $k$-limited abstractions.
}
\end{table}
