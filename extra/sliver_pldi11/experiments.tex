\Section{experiments}{Experiments}

In this section, we apply the Prune-Refine algorithm (\refsec{algorithm}) to
$k$-object-sensitivity for our three clients (\refsec{clients}):
downcast safety checking (\downcast),
monomorphic call site inference (\monosite),
and race detection (\race).
Our main empirical result is that across different clients and benchmarks,
pruning is effective at curbing the exponential growth,
which allows us to run analyses on finer abstractions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{setup}{Setup}

% Benchmark/setup
Our experiments were performed using IBM J9VM 1.6.0 on 64-bit Linux machines.
All analyses were implemented in Chord,
an extensible program analysis framework for Java bytecode,\footnote{{\tt{http://code.google.com/p/jchord/}}}
which uses the BDD Datalog solver {\tt bddbddb} \cite{Whaley2007}.
We evaluated on \numBenchmarks\ Java benchmarks shown in \reftab{benchmarks}.
In each run, we allocated 8GB of memory and terminated the process when it ran out of memory.

\begin{table*}
\centering
\input figures/stats.tex
\caption{Benchmark characteristics:
the number of classes,
number of methods,
total number of bytecodes in these methods,
and number of allocation sites ($|\H|$)
deemed reachable by 0-CFA.
\label{tab:benchmarks}}
\end{table*}

% Algorithms
We experimented with various combinations of abstractions and refinement
algorithms (see \reftab{algorithms}).  As a baseline, we consider running an analysis
with a full abstraction $\alpha$ (denoted $\Full(\alpha)$).
For $\alpha$, we can either use $k$-limited abstractions
($(\klimabs_k)_{k=1}^\infty$), in which case we recover ordinary $k$-object-sensitivity,
or the barely-repeating variants ($(\klimdabs_k)_{k=1}^\infty$).
We also consider the site-based refinement algorithm of \cite{liang11minimal},
which considers a sequence of abstractions $\balpha = (\balpha_0, \balpha_1, \dots)$
but stops refining sites which have been deemed irrelevant.  This algorithm is denoted $\Site(\balpha)$.

As for the new methods that we propose in this paper,
we have $\PR(\balpha)$, which corresponds to the Prune-Refine (\PR) algorithm using a sequence of abstractions $\balpha$ with no pre-pruning;
and $\PR(\balpha,\hclass)$, which performs pre-pruning using $\beta_t = \alpha_t
\circ \hclass$ for $t = 0, 1, 2, \dots$.  We consider three choices of $\hclass$
which use different kinds of type information ($\hclassIs,\hclassHas,\hclassIsHas$)

In our implementation of the Prune-Refine algorithm, we depart slightly from our
presentation.  Instead of maintaining the set of relevant input tuples as
determined by our $k$-limited Datalog program across iterations, we instead
maintain the allocation site chains which were involved in any relevant input
tuple.  This choice results in more conservative pruning, but reduces the
amount of state that we have to keep.
We can modify the original Datalog program so that the Prune-Refine algorithm
is consistent with this implementation by introducing a new input relation
$\dl{active}(c)$ for $c \in \H^*$, encoding existing $\ext$ input relations as rules
with zero source terms, and adding $\dl{active}(c)$ for each existing
rule that uses a chain-valued variable.  Computing the
relevant input tuples in this modified Datalog program corresponds exactly to
computing the set of relevant allocation site chains.

\begin{table}
\[
\begin{array}{ll}
\text{\bf Abstractions} \\
\Klimabs = (\klimabs_k)_{k=1}^\infty   & \defn{$k$-limited abstractions \refeqn{klimitedAbstraction}} \\
\Klimdabs = (\klimdabs_k)_{k=1}^\infty & \defn{barely-repeating $k$-limited abstractions \refeqn{barelyAbstraction}} \\
\hclassIs                              & \defn{abstraction using type of allocation site} \\
\hclassHas                             & \defn{abstraction using type of containing class} \\
\hclassIsHas                           & \defn{abstraction using both types} \\
\\
\text{\bf Algorithms} \\
\Full(\alpha)         & \defn{standard analysis using an abstraction $\alpha$} \\
\Site(\balpha)        & \defn{site-based refinement \cite{liang11minimal} on abstractions $\balpha$} \\
\PR(\balpha)          & \defn{\PR\ algorithm using $\balpha$, no pre-pruning} \\
\PR(\balpha,\hclass)  & \defn{\PR\ algorithm using $\balpha$, using $\balpha\circ\hclass$ to pre-prune} \\
\end{array}
\]
\caption{\label{tab:algorithms} Shows the abstractions and algorithms that we evaluated empirically.
For example,
$\PR(\Klimdabs,\hclassIsHas)$
means running the Prune-Refine algorithm on the barely-repeating $k$-limited abstraction ($\alpha_k = \klimdabs_k$),
using a composed abstraction based on the type of an allocation site ($\hclassIs$) 
and the type of the declaring class ($\hclassHas$) to do pre-pruning
(specifically, $\beta_k = \klimdabs_k \circ \hclassIsHas$).
}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{results}{Results}

We ran four analyses using $k$-limited abstractions, seeing
how far we could increase $k$ until the analyses ran out of memory.
For each analysis, we also measured the number of input tuples given to
the Datalog solver; this quantity is denoted as $|A_t'|$ (see
\reffig{pseudocode}).  In this section, the number of iterations $t$ is the same as the $k$ value.

\reffig{absSizeGraph} plots the number of tuples $|A_t'|$ as a function of
number of iterations $t$ for our four algorithms.  We see that the
non-pruning algorithms completely hit a wall after a few iterations, with the
number of tuples exploding exponentially.  On most benchmark-client pairs, the
pruning methods are able to continue increasing $k$ much farther, but on several
pairs, pruning only manages to increase $k$ by one beyond the non-pruning algorithms.
We also observed that pruning does yield speedups, although these are
less pronounced than the differences in the number of tuples.  Nonetheless, pruning
overcomes the major bottleneck, which is that standard $k$-limited analyses run
out of memory even for moderate $k$.  By curbing the growth of the
number of tuples, pruning makes it possible to run some analyses at all.

However, there are several caveats with pruning:
First, we are using BDDs, which can actually handle large numbers of tuples
so long as they are structured; pruning destroys some of this structure,
yielding less predictable running times.  Second, pruning requires solving
the transformed Datalog program for computing $\bP(X)$, which is more expensive than the original Datalog program.
Finally, we have to solve the Datalog program several times, not just once.
These three caveats also apply to the site-based refinement algorithm of \cite{liang11minimal},
so pruning is a strict improvement over that.

\FigStar{figures/absSizeGraph}{0.35}{absSizeGraph}{For each client/benchmark pair,
we show the growth of the number of input tuples $|A_t'|$ across iterations.
Recall that $|A_t'|$ is the number of tuples fed into the Datalog program.
\reftab{algorithms} describes the four methods.
We see that the methods that prune drastically cut down the number of input tuples
by many orders of magnitude.
}

\reftab{sizeRatio} provides more details on the quantitative impact of pruning in our
best instantiation of the Prune-Refine algorithm ($\PR(\bpi, \hclassIsHas)$).  We see that pre-pruning has a
significant positive impact: we are able to perform eliminate many tuples by just operating on the coarser
level of types rather than allocation sites (see the $\frac{|\tilde B_t|}{|B_t|}$ column).  Many of the tuples are pruned
at the type level, and the results of this pruning carry over to the original
$k$-limited abstraction (see the $\frac{|\tilde A_t'|}{|A_t|}$ column).

\begin{table}
\centering
\small
\input figures/sizeRatio
\caption{\label{tab:sizeRatio} Shows the shrinking and growth of the
number of tuples during the various pruning and refinement operations (see \reffig{pseudocode}) for 
our best method $\PR(\Klimabs,\hclassIsHas)$ across all the clients and benchmarks, averaged across iterations.
The columns are as follows:
First, $\frac{|B_t|}{|A_t|}$ measures the number of tuples after projecting
down to the auxiliary abstraction $\beta_t = \klimdabs_t \circ \hclassIsHas$ for pre-pruning; note that running the analysis
using types instead of allocation sites is much cheaper.
Next, $\frac{|\tilde B_t|}{|B_t|}$ shows the fraction of abstract values kept during pre-pruning;
When we return from types to allocation sites, see that the effect of pre-pruning essentially carries over
($\frac{|A_t'|}{|A_t|}$).
After pre-pruning, pruning kept $\frac{|\tilde A_t|}{|A_t|}$ of the chains.
Finally, $\frac{|A_{t+1}|}{|A_t|}$ measures the ratio between iterations,
which includes both pruning and refinement.  Note that there is still almost a three-fold
growth of the number of tuples (on average), but this growth would have been much more unmanageable
without the pruning.
}
\end{table}

So far, we have been using the $k$-limited abstraction; we now compare this abstraction
with the barely-repeating $k$-limited abstraction introduced in \refsec{barelyAbstraction}.
As \reffig{barelyRepeatingEffect} shows, we found that when we can increase the $k$
value, the barely-repeating $k$-limited abstraction reduces the number of tuples, but in most
cases, it does not improve scalability.  The reason is that the
barely-repeating abstraction curbs refinement, but often, and somewhat
paradoxically, it is exactly the refinement which enables us to prune more.

\FigStar{figures/barelyRepeatingEffect}{0.35}{barelyRepeatingEffect}{
Shows the 5 (out of 15) client/benchmark pairs for which using the
barely-repeating $k$-limited abstraction ($\Klimdabs$) allows one to increase
$k$ much more than the plain $k$-limited abstraction ($\Klimabs$).
On the other 10 client/benchmarks where $k$ cannot get very large,
limiting repetitions is actually slightly worse in terms of scalability.
Three of the lines for $\Klimdabs$ stop early, not because the algorithm runs out of memory,
but because the algorithm has actually converged and increasing $k$ would have no effect.
}

Finally, \reftab{numUnproven} shows the effect on the number of queries proven.
While pruning enables us to increase $k$ much more than before, it turns out
that our particular analyses for these clients saturate quite quickly, so
over all the clients and benchmarks,
we were only able to prove two queries more than using the non-pruning techniques.
On the surface, these findings seem to contradict \cite{liang10abstraction},
which showed a sharp increase in precision around $k=4$ for $k$-CFA.  However,
this discrepancy merely suggests that our flow-insensitive analyses are simply
limited: since \cite{liang10abstraction} obtains a lower bound, we know for
sure that low $k$ values are insufficient; the fact that we don't see an
increase in precision suggests that the non-$k$-related aspects of our analyses
are insufficient.  However, given that our pruning approach is general, it
would be interesting to tackle other aspects of program analysis such as
flow-sensitivity.

\begin{table}
\small
\centering
\input figures/numUnproven
\caption{\label{tab:numUnproven} The number of unproven queries (unsafe
downcasts, polymorphic sites, races) for each of the clients and benchmarks
over the first five iterations.
All analyses obtain the exact results on iterations where they obtain an answer.
Bolded numbers refer to $k$ values reached by $\PR(\Klimabs,\hclassIsHas)$
but not by any non-pruning algorithm.
While pruning enables to increase $k$ more, we get strictly more precise results
for only two of the client/benchmark pairs (\downcast/\hedc\ and \downcast/\lusearch).
This points out inherent limitations of this family of $k$-limited abstractions.
}
\end{table}
