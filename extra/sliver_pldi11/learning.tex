\Section{randSolutions}{Machine Learning Approach}

We now turn to the main contribution of this paper, which is a machine learning
approach for finding minimal abstractions.  We present two algorithms:
$\StatRefine$, which refines an abstraction by iteratively adding components
(\refsec{statRefine}) and $\ActiveCoarsen$, which coarsens an abstraction by removing components
(\refsec{activeCoarsen}).

These two algorithms at a high-level parallel their deterministic counterparts,
$\DatalogRefine$ and $\ScanCoarsen$, presented in the previous section.
However, there are two important distinctions:
(i) these algorithms find a minimal abstraction much more efficiently by exploit {\em sparsity}, the
property that a minimal abstraction is much smaller than the full set of components;
(ii) randomization is the key tool that allows us to exploit sparsity.

$\StatRefine$ is a Monte Carlo algorithm (the running time is fixed
there is some probability we will not find a minimal abstraction),
$\ActiveCoarsen$ is a Las Vegas algorithm (the running time is random, but we
are guaranteed to find a minimal abstraction).

For clarity of presentation, we first focus on the case where we have one
query, coming back to the issue in \refsec{multiQueries}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{statRefine}{Refinement via Statistical Learning}

We call a component $j \in \J$ {\em relevant} if $j$ appears in some minimal
abstraction.
The main objective of $\StatRefine$ is to identify a component
guaranteed to be part of some minimal abstraction.
This is done by sampling $n$ independent random abstractions and running the
static analysis $\bF$ on them.  The component $j$ associated with
the most number of proven queries is then added to the abstraction, and we
recurse until we can prove the query.
The pseudocode of the algorithm is given in \reffig{statRefine}.

% Algorithm
\begin{figure}[t]
\begin{center} \scalebox{0.95}{\framebox{ \begin{minipage}{3.3in} % Begin model
\begin{center} Refinement via Statistical Learning \end{center}
Parameters: \\
\ind $\alpha$: refinement probability \\
\ind $n$: number of training examples \\
$\Sample(\alpha, \Jlb, \Jub)$: \\
\ind $\ba \leftarrow \Jlb$ \\
\ind for each component $j \in \Jub \backslash \Jlb$: \\
\ind\ind $\ba_j \leftarrow 1$ with probability $\alpha$ \\
\ind return $\ba$ \\
\\
$\StatRefine(\alpha, n, \Jlb, \Jub)$: \\
\ind if $\bF(\Jlb) = 1$: return $\Jlb$ \\
\ind for $i = 1, \dots, n$: \aside{create training examples} \\
\ind\ind $\ba^{(i)} \leftarrow \Sample(\alpha, \Jlb, \Jub)$ \\
\ind $j^* \leftarrow \argmax_{j \not\in \Jlb} |\{ i : \ba^{(i)}_j = 1, \bF(\ba^{(i)}) = 1 \}|$ \\
\ind return $\StatRefine(\alpha, n, \Jlb \cup \{ j^* \})$
\end{minipage} }} \end{center} % End model
\caption{\label{fig:statRefine} Algorithm for finding a minimal abstraction
by iteratively adding relevant components determined via statistical learning.
}
\end{figure}

% What we actually implemented
% Algorithm
%\begin{figure}[t]
%\begin{center} \scalebox{0.95}{\framebox{ \begin{minipage}{3in} % Begin model
%\begin{center} Refinement via Statistical Learning \end{center}
%Parameters: \\
%\ind $\alpha$: refinement probability \\
%\ind $n$: number of training examples \\
%\\
%$\StatRefine(\alpha, n)$: \\
%\ind for $i = 1, \dots, n$: \aside{create training examples} \\
%\ind\ind for each component $j \in \J$: \\
%\ind\ind\ind set $\ba_j^{(i)} = 1$ with probability $\alpha$ \\
%\ind $\sT \leftarrow \{ \ba^{(i)} : i = 1, \dots, n \}$ \\
%%\ind $\sT^+ \leftarrow \{ \ba^{(i)} : \bF(\ba^{(i)}) = 1 \}$ \aside{positive examples} \\
%%\ind $\sT^- \leftarrow \{ \ba^{(i)} : \bF(\ba^{(i)}) = 0 \}$ \aside{negative examples} \\
%%\ind $\hat\ba \leftarrow \bzero$ \aside{start with coarsest abstraction} \\
%%\ind while $\sT^- \neq \emptyset$: \\
%%\ind\ind $j^* \leftarrow \argmax_{j \in \J} \Score(\sT^+, \sT^-, j)$ \\
%%\ind\ind $\sT^+ \leftarrow \{ \ba \in \sT^+ : \ba_{j^*} = 1 \}$ \\
%%\ind\ind $\sT^- \leftarrow \{ \ba \in \sT^- : \ba_{j^*} = 0 \}$ \\
%%\ind\ind $\hat\ba_{j^*} \leftarrow 1$ \\
%%\ind return $\hat\ba$
%\ind define $\hat p(\ba, y)$ in terms of $\sT$ (see \refeqn{jointDistrib}) \\
%\ind $J \leftarrow \emptyset$ \\
%\ind while $\hat p(y = 1 \mid \ba_J = \bone_J) < 1$: \\
%\ind\ind $j^* \leftarrow \argmax_{j \in \J} I(\hat p, J \cup \{ j \})$ \\
%\ind\ind $J \leftarrow J \cup \{ j^* \}$ \\
%\ind return $\bone_J$
%\end{minipage} }} \end{center} % End model
%\caption{\label{fig:statRefine} Algorithm for finding a minimal abstraction via statistical learning.}
%\end{figure}

Whereas $\DatalogRefine$ inspects the Datalog program backing $\bF$ to
overapproximate the set of relevant components, $\StatRefine$ relies instead on
statistical correlations between relevant components and the output of $\bF$.
As \refthm{statRefine} will show, with high probability,
a relevant component can be found with $n$ calls to $\bF$, where $n$ is
only logarithmic in the number of components $|\Jub|$.
This logarithmic dependence results from exploiting sparsity.

% Main challenge
We must also ensure that $n$ depends only polynomially on $s$.
The main technical challenge is to set $\alpha$ properly to achieve this.
To appreciate this problem,
suppose $\bF(\ba)$ were a simple conjunction ($\bF(\ba) = 1$ iff $\ba_j = 1$ for each relevant $j$).
If we set $\alpha$ too small, then it would take an exponential number of examples ($(\inv{\alpha})^s$ in expectation)
to even see a positive example.
\refthm{statRefine} shows that if $\alpha$ is set properly, then we obtain the desired polynomial dependence.

\begin{theorem}[Properties of $\StatRefine$]
\label{thm:statRefine}
Let $s$ be the number of relevant components in $\J$ (those that appear in some minimal abstraction).
Suppose we set the refinement probability $\alpha = (\frac{s}{s+1})^s$ 
and obtain $n = \Omega(s^2 (\log |\Jub \backslash \Jlb| + \log (s/\delta)))$ training examples from $\bF$ each iteration.
Then with probability $1-\delta$,
$\StatRefine$ outputs a minimal abstraction.
The total running time is $O(s^3 (\log |\Jub \backslash \Jlb| + \log
(s/\delta)))$.
\end{theorem}

\begin{proof}
Without loss of generality, let $\Jub = \emptyset$.
Note that the algorithm will run for at most $s$ iterations.
If we set $n$ so that $\StatRefine$ chooses an irrelevant component with probability at most $\delta/s$,
then the algorithm will succeed with probability (at least) $1-\delta$.

Let us now focus on one iteration.
The main idea is that for sufficiently large $n$, 
a component $j$ that is relevant (part of some minimal abstraction)
will be more correlated with proving the query ($\bF(\ba) = 1$) than one that is irrelevant,
which enables to pick it out with high probability.

Let $\Jrel$ be the set of relevant components with $|\Jrel| = s$.
Let $j^+ \in \Jrel$ be a relevant component and $j^- \not\in \Jrel$ be an irrelevant component.
For each training example $\ba^{(i)}$, define
$X_i = \bF(\ba^{(i)}) (\ba^{(i)}_{j^-} - \ba^{(i)}_{j^+})$,
which is (i) 1 if $\bF(\ba^{(i)}) = 1$ and component $j^-$ is active ($\ba^{(i)}_{j^-} = 1$)
but $j^+$ is not,
(ii) $-1$ if $\bF(\ba^{(i)}) = 1$ and the reverse is true,
(iii) 0 otherwise.
Note that $j^-$ is favored over $j^+$ exactly when $\inv{n} \sum_{i=1}^n X_i > 0$.
We can bound this bad event using Hoeffding's inequality.\footnote{Hoeffding's inequality: if $X_1, \dots, X_n$ are i.i.d.~random variables 
with $a \le X_i \le b$, then
$p(\inv{n} \sum_{i=1}^n X_i > \E[X_i]+\epsilon) \le \exp\pc{-\frac{2 n \epsilon^2}{(b-a)^2}}$.}
The mean is $\E[X_i] = p(\bF(\ba)=1, \ba_{j^-}=1) - p(\bF(\ba)=1, \ba_{j^+}=1)$,
and the bounds are $a = -1$ and $b = +1$.
Setting $\epsilon = -\E[X_i]$, we get:
\begin{align}
\label{eqn:joverj}
p(j^- \text{ favored over } j^+) \le e^{-n\epsilon^2/2}, \quad j^+ \in \Jrel, j^- \not\in \Jrel.
\end{align}
The probability that an irrelevant component is chosen is at most
the probability that some $j^- \not\in\Jrel$ is favored over a fixed $j^+ \in \Jrel$.
The latter probability
can be bounded by summing the right-hand side of \refeqn{joverj} over $|\Jub \backslash \Jrel|$ such $j^-$.
Rearranging terms, we find that setting $n$ as follows sufficient for
correctness of the full algorithm (with probability $1-\delta$):
\begin{align}
\label{eqn:delta}
\delta/s \le |\Jub| e^{-n\epsilon^2/2}
\Rightarrow n \ge \frac{2(\log |\Jub| + \log (s/\delta))}{\epsilon^2}.
\end{align}

Now it remains to lower bound $\epsilon$, which intuitively represents the gap
(in the amount of correlation with proving the query) between a relevant
component and an irrelevant one.
Note that $p(\ba_j = 1) = \alpha$ for any component $j \in \Jub$.
Also, an irrelevant component is independent of $\bF(\ba)$, so $p(\bF(\ba) = 1 \mid \ba_{j^-} = 1) = p(\bF(\ba) = 1)$.
Using these two facts, we have:
\begin{align}
\label{eqn:epsilon}
\epsilon = \alpha (p(\bF(\ba) = 1 \mid \ba_{j^+} = 1) - p(\bF(\ba) = 1)).
\end{align}

Note that $\bF$ can always be written as a DNF formula over $s$ variables;
let $C$ be the set of clauses of the DNF where each clause $c \in C$ is a subset of $\Jrel$.
For example, $C = \{ \{ 1,2 \}, \{ 3 \} \}$ corresponds to $\bF(\ba) = (\ba_1
\wedge \ba_2) \vee \ba_3$.\footnote{Note that each clause corresponds to a
minimal abstraction.}
Let $C_j = \{ c \in C : j \in c \}$ be the clauses that depend on component $j$.
Then, divide $p(\bF(\ba) = 1)$ into two parts,
one that depends on $j^+$ and one that does not:
\begin{align}
p(\bF(\ba) = 1) = & \, p(\exists c \in C_{j^+}, c \subset \ba \text{ and } \forall c \not\in C_{j^+}, c \not\subset \ba) + \nonumber \\
                  & \, p(\exists c \not\in C_{j^+}, c \subset \ba).
\end{align}
Computing $p(\bF(\ba) = 1 \mid \ba_{j^+} = 1)$ is similar; the only difference is that
the first term is multiplied by $\inv{\alpha}$.
Plugging these two results back into \refeqn{epsilon} yields:
\begin{align}
\label{eqn:epsilonFinal}
\epsilon = (1 - \alpha) p(\exists c \in C_{j^+}, c \subset \ba \text{ and } \forall c \not\in C_{j^+}, c \not\subset \ba).
\end{align}
Intuitively, $j^+$ has a distinct advantage as long as clauses in $C_{j^+}$
are activated but clauses not in $C_{j^+}$ are not.

Recall we want to lower bound \refeqn{epsilonFinal} over all possible $\bF$ (equivalently, $C$).
It turns out that the worst possible $C$ is obtained by either having $s$
disjoint clauses ($C = \{ \{ j \} : j \in \Jrel \}$) or one clause ($C = \{ \Jrel \}$).
The intuition is that
if $C$ has $s$ clauses, there are many ($s-1$) opportunities for some $c \not\in C_{j^+}$ to activate,
making it hard to realize that $j^+$ is relevant;
in this case, $\epsilon = (1-\alpha) \alpha (1-\alpha)^{s-1}$.
If $C$ has one clause, then it is very hard (probability $\alpha^s$) to even activate this clause;
in this case, $\epsilon = (1-\alpha) \alpha^s$.

Let us focus on the one clause case.
Recall that we need to set $\alpha$ to avoid an exponential dependence on $s$.
We can maximize $\epsilon$ with respect to $\alpha$
by setting $\frac{d \epsilon}{d \alpha} = 0$ and solving for $\alpha$.
Doing this yields $\alpha = \frac{s}{s+1}$ as the optimal value.
Plugging this value back into the expression for $\epsilon$, we get that
$\epsilon = \inv{s+1} (\frac{s}{s+1})^s$.
The second factor can be lower bounded by $e^{-1}$,
so $\epsilon^{-2} = O(s^2)$.
Combining this with \refeqn{delta} completes the proof.
\end{proof}

%Define the {\em empirical joint distribution} over abstractions and their output:
%\begin{align}
%\label{eqn:jointDistrib}
%\hat p(\ba, y) = \inv{|\sT|} \sum_{\ba' \in \sT} \1[\ba = \ba', y = \bF(\ba')].
%\end{align}
%
%At the end,
%we output a set of components $J$ such that $\hat p(y = 1 \mid \ba_J = \bone) = 1$,
%that is, there is no evidence that $\bF(\bone_J) = 0$.
%We also want to find a $J$ which has a lot of support ($p(\ba_J)$).
%Note that this naturally favors small $J$, as desired, since we want a minimal abstraction.
%Formally, our optimization problem is as follows:
%\begin{align}
%\max_{J : \hat p(y = 1 \mid \ba_J = \bone_J) = 1} \hat p(\ba_J).
%\end{align}
%
%Greedy algorithm that maximizes:
%\begin{align}
%I(p, J) = p(\ba_J = \bone_J, y = 1) \log \frac{p(y = 1 \mid \ba_J = \bone)}{p(y = 1)}.
%\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{activeCoarsen}{Coarsening via Active Learning}

% Introduction
% Algorithm summary 
We now present an algorithm, $\ActiveCoarsen$,
which takes an abstraction $\Jub$ which can prove the query,
and removes components from $\Jub$ while still preserving correctness.
The pseudocode of the algorithm is given in \reffig{activeCoarsen}.
Given an upper bound $\Jub$ which proves the
query, we sample a random abstraction smaller than $\Jub$.
If the sampled abstraction $\ba$ works ($\bF(\ba) = 1$),
then we can safely remove all components not in $\ba$
and recurse.  Otherwise, we try again.

% Algorithm
\begin{figure}[t]
\begin{center} \scalebox{0.95}{\framebox{ \begin{minipage}{3.3in} % Begin model
\begin{center} Coarsening via Active Learning \end{center}
Parameters: \\
\ind $\alpha$: refinement probability \\
\ind $s$: upper bound on complexity of minimal abstraction \\
\\
$\ActiveCoarsen(\alpha, s, \Jlb, \Jub)$: \\
\ind if $|\Jub \backslash \Jlb| \le s+1$: return $\ScanCoarsen(\Jlb, \Jub)$ \\
\ind $\ba \leftarrow \Sample(\alpha, \Jlb, \Jub)$ \\
\ind if $\bF(\ba) = 1$: \aside{run static analysis} \\
\ind\ind return $\ActiveCoarsen(\alpha, s, \Jlb, \ba)$ \aside{reduced} \\
\ind else: \\
\ind\ind return $\ActiveCoarsen(\alpha, s, \Jlb, \Jub)$ \aside{try again} \\
\end{minipage} }} \end{center} % End model
\caption{\label{fig:activeCoarsen}
$\ActiveCoarsen$ returns a minimal abstraction $\ba$ by iteratively removing a random
$\alpha$-fraction of the components from an upper bound.
$\Sample$ is defined in \reffig{statRefine}.
}
\end{figure}

% Intuition
Recall that $\ScanCoarsen$ removed one component at a time from an upper bound,
which required an exorbitant $O(|\Jub|)$ calls to the static analysis.
The key idea behind $\ActiveCoarsen$ is to remove a constant fraction of
components each iteration.  We then only need $O(\log_{1/\alpha} |\Jub|)$ iterations.

However, there is still a probability that we will not be able to prove the
query ($\bF(\ba) = 0$).  To appreciate this problem, suppose
$\bF(\ba) = (\ba^* \subset \ba)$ for some unknown set $\ba^*$ with $|\ba^*| = s$.
Then there is only a $\alpha^s$ probability of sampling a random abstraction
$\ba$ that proves the query ($\bF(\ba) = 1$).
The expected number of trials is thus $(\inv{\alpha})^s$, which has an
unfortunate exponential dependence on $s$.
This discussion reveals a tradeoff: setting $\alpha$ too small results in too many trials per iteration,
but setting $\alpha$ too large results in too many iterations.
Fortunately, as \refthm{activeCoarsen} shows, we can balance the two
to yield an efficient algorithm.

\begin{theorem}[Properties of $\ActiveCoarsen$]
\label{thm:activeCoarsen}
Let $s \ge |\ba \backslash \Jlb|$, where $\ba$ is
the largest minimal abstraction finer than $\Jlb$.
If we set the refinement probability $\alpha = e^{-1/s}$,
the expected number of calls to the analysis $\bF$ made by $\ActiveCoarsen$
is $O(s \log |\Jub \backslash \Jlb|)$.
%is at most $e s \log (|\Jub \backslash \Jlb|-s) + s$.
\end{theorem}

% Analogy with binary search
%Note that the number of calls scales linearly with the target size $s$ and
%logarithmically with $|\ba|$.  Intuitively, the algorithm can be thought of as a
%generalized binary search on $\J$, where the random $\ba'$ determines a partitioning of $\J$ into two halves
%(rather than a split point determining the two halves), and we are searching
%for $s$ elements rather than one.
%In view of this connection, the $\log |\ba|$ is the natural dependence.

\begin{proof}
Without loss of generality, let $\Jlb = \bzero$.
Let $T(r)$ be the expected number of calls to $\bF$
that $\ActiveCoarsen$ takes, where $r = |\Jub| - s$.
We compute $T(r)$ as follows:
For the base case, $T(1) = s+1$,
since when $|\ba| = s+1$, the algorithm calls $\ScanCoarsen$, which makes $s+1$ calls to $\bF$.
For the inductive case,
let $\ba \leftarrow \Sample(\alpha, \Jlb, \Jub)$ be a random binary vector.
If $\bF(\ba) = 1$, we reduce to a sub-problem of size $|\ba|$;
otherwise, we are left with the same problem of size $r$:
\begin{align}
T(r) &= 1 + \E[\bF(\ba) T(|\ba|-s) + (1-\bF(\ba)) T(r)].
\end{align}
By assumption, there exists an abstraction $\ba^*$ of size $s$ that proves the query.
Define $\bG(\ba) = (\ba^* \subset \ba)$ and note that $\bG(\ba) \le \bF(\ba)$,
since $\ba^*$ is only one way to prove the query.
Since $T(|\ba|-s) \le T(r)$,
we can get an upper bound by replacing $\bF$ with $\bG$:
\begin{align}
T(r) &\le 1 + \E[\bG(\ba) T(|\ba|-s) + (1-\bG(\ba)) T(r)].
\end{align}
For the first term inside the expectation, $\bG(\ba)$ constrains $s$ components of $\ba$ to 1 (with probability $\alpha^s$)
and the rest are determined by $r$ independent coin flips with success probability $\alpha$.
The second term is simply the probability that $\bG(\ba) \neq 1$ times $T(r)$.
Thus:
\begin{align}
T(r) &\le 1 + \alpha^s \E[T(N)] + (1-\alpha^s) T(r)],
\end{align}
where $N$ is a binomial random variable which has expectation $\alpha r$.
Rearranging terms:
\begin{align}
T(r) &\le \E[T(N)] + \alpha^{-s}.
\end{align}
Since $T(r)$ is concave, we can apply Jensen's inequality, yielding:
\begin{align}
T(r) &\le T(\E[N]) + \alpha^{-s} = T(\alpha r) + \alpha^{-s}.
\end{align}
Solving the recurrence, we obtain:
\begin{align}
\label{eqn:Tr}
T(r) \le \frac{\alpha^{-s} \log r}{\log \alpha^{-1}} + s+1.
\end{align}
From \refeqn{Tr}, we can see the tradeoff between reducing the
number of iterations (by increasing $\log \alpha^{-1}$)
versus reducing the number of trials (by decreasing $\alpha^{-s}$).

We now set $\alpha$ to minimize the upper bound.
Differentiate with respect to $x = \alpha^{-1}$ and set the derivative to zero:
$\frac{s x^{s-1}}{\log x} - \frac{x^{s-1}}{\log^2 x} = 0$.
Solving this equation yields $\alpha = e^{-1/s}$.
Plugging this value back into \refeqn{Tr} yields
$T(r) = e s \log r + s+1 = O(s \log |\Jub\backslash\Jlb|)$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{adaptRefineProb}{Adapting the Refinement Probability}

% Problem
Up until now, we have assumed that the size of the maximum abstraction $s$ is
known, and indeed \refthms{statRefine}{activeCoarsen} depend crucially on
setting $\alpha$ properly in terms of $s$.
In practice, $s$ is unknown, so we would like a mechanism for setting $\alpha$
without this knowledge.

% Intuition: want fixed p(y=1)
The intuition is that setting $\alpha$ properly ensures that queries are proven
with a probability $p(\bF(\ba) = 1)$ bounded away from 0 by a constant.
Indeed, in $\StatRefine$, following the prescribed setting of $\alpha$,
we get $p(\bF(\ba) = 1) = (\frac{s}{s+1})^s$;
in $\ActiveCoarsen$, we get $p(\bF(\ba) = 1) = e^{-1}$.
(Remarkably, $(\frac{s}{s+1})^s$ is lower bounded by $e^{-1}$ and tends
exactly to $e^{-1}$ as $s \to \infty$.)

% Adaptive
The preceding discussion motivates a method that keeps $p(\bF(\ba) = 1) \approxeq t$,
where $t = e^{-1}$ is the target probability.
We can accomplish this by adapting $\alpha$ as we get new examples.
The algorithm we will derive is simple: if $\bF(\ba) = 1$, we decrease
$\alpha$; otherwise, we increase $\alpha$.  But by how much?

To avoid boundary conditions, we parametrize $\alpha = \sigma(\theta) = (1 + e^{-\theta})^{-1}$,
which maps $-\infty < \theta < \infty$ to $0 < \alpha < 1$.
For convenience, let us define $g(\theta) = p(\bF(\ba) = 1)$.
Now consider the minimizing the following function:
\begin{align}
\sO(\theta) = \half (g(\theta) - t)^2.
\end{align}
Clearly, the optimum value (zero) is obtained by setting $\theta$ so that $g(\theta) = t$.
We can optimize $\sO(\theta)$ by updating its gradient:
\begin{align}
\theta \leftarrow \theta - \eta \frac{d\sO}{d\theta}, \quad \frac{d\sO}{d\theta} = (g(\theta) - t) \frac{d g(\theta)}{d \theta},
\end{align}
where $\eta$ is the step size.
Of course we cannot evaluate $g(\theta)$,
but the key is that we can obtain unbiased samples of $g(\theta)$ by evaluating $\bF(\ba)$ (which we needed to do anyway);
specifically, $\E[\bF(\ba)] = g(\theta)$.
We can therefore replace the gradient with a stochastic gradient, a classic
technique which is known to have nice theoretical properties \cite{munro51stochastic}.
We note that $\frac{d g(\theta)}{d \theta} > 0$, so we absorb it into the step size $\eta$.
This leaves us with the following rule for updating $\theta$ given a random $\ba$:
\begin{align}
\theta \leftarrow \theta - \eta (F(\ba) - t).
\end{align}
We set $\eta = 0.1$, which worked well in practice.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{multiQueries}{Multiple Queries and Parallelization}

% Multiple queries
So far, we have presented all algorithms for one query.
Given multiple queries, we could just solve each query independently,
but this can be quite inefficient.
We therefore adopt the following {\em lazy splitting} strategy:
We start with all the queries in a group and start running whichever algorithm;
if we find that the behavior on those queries ever diverges,
we split up the group to maintain behavioral homogeneity within group.
Specifically, we maintain a partition of $\Q$ into a set of groups $G$,
where each $g \in G$ is a subset of $\Q$.
We run the algorithm independently for each $g \in G$.
If at any point in time we find $\bF(\ba)_q$ is not the same for all $q \in g$,
then we break $g$ into
$g_0 = \{ q \in g : \bF(\ba)_q = 0 \}$ and
$g_1 = \{ q \in g : \bF(\ba)_q = 1 \}$,
and set $G$ to $(G \backslash \{ g \}) \cup \{ g_0, g_1 \}$.
In practice, when the algorithm terminates,
many $g$ will still contain multiple queries,
meaning that they share the same minimal abstraction.
In \refsec{experiments}, we see that the number of final groups is much smaller than
the number of queries.

Using this lazy splitting strategy, we see that $\StatRefine$ has a distinct
advantage over $\ActiveCoarsen$, because the $n$ training
examples sampled in the first iteration can be used across all queries (there
is initially only one group), whereas for $\ActiveCoarsen$, the queries become
quickly fragmented across groups.

% Parallelization
Our algorithms have been presented for a single processor,
but parallelization is also possible.
$\StatRefine$ is very easy to parallelize because the $n$ training examples
are gathered independently.
It is less obvious how to parallelize $\ActiveCoarsen$, because of the sequential dependence of calls to $\bF$.
But we can use the following intuition:
the probability that a random abstraction will prove the query is (at least) $\alpha^s$,
so if evaluate $m$ random abstractions on $m$ processors,
the probability that at least one of them will succeed in proving the query is
$1-(1-\alpha^s)^p$, which is larger.
Therefore, we can afford to set $\alpha$ smaller than before, which will provide a
larger reduction if one of the $p$ random abstractions is successful.
%essentially rebalancing the tradeoff between the numerator and denominator in \refeqn{Tr}.
We do not need to compute $\alpha$,
but just set it adaptively so that the target probability is
$t = e^{-1}/p$ rather than $t=e^{-1}$.

\begin{table*}
\begin{center}
\begin{tabular}{l|llll}
Algorithm      & Minimal                     & Correct                      & Parallelizes easily & \# calls to $\bF$ \\ \hline
\DatalogRefine & no                          & yes                          & yes                 & $O(1)$ \\
\ScanCoarsen   & yes                         & yes                          & no                  & $O(|\J|)$ \\ \hline
\StatRefine    & prob. $1-\delta$            & prob. $1-\delta$             & yes                 & $O(s^3 (\log |\J| + \log (s/\delta))$ \\
\ActiveCoarsen & yes                         & yes                          & no                  & $O(s \log |\J|)$ [in expectation] \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:algSummary}
Summary showing the strengths and weaknesses of the four algorithms for
finding minimal abstractions.  Note that the two machine learning algorithms
have only a logarithmic dependence on $|\J|$, the total number of components,
and a polynomial dependence on the number of relevant components $s$ in a minimal abstraction.
}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Subsection{hybrid}{Discussion of Algorithms}

\reftab{algSummary} summarizes the properties of the four different algorithms
we have presented in this paper.
One of the key advantages of the learning-based approaches
($\StatRefine$ and $\ActiveCoarsen$) is that they take advantage of {\em
sparsity}, the property that only $s$ components of the $|\J|$ components are
relevant, where $s \ll |\J|$.  As a result, both algorithms depend logarithmically on $|\J|$,
and both involve sampling random abstractions by including each component with
probability $\alpha$.

Also, in both cases, to avoid an exponential dependence on $s$,
it was important to set the probability $\alpha$ properly;
for $\StatRefine$, so that characteristics of an irrelevant component is sufficiently different
from that of a relevant component;
for $\ActiveCoarsen$, so that the probability of obtaining a successful
reduction of the problem is sufficiently large.

% Hybrid algorithm
\paragraph{A Hybrid Approach}

% Differences
The two machine learning algorithms are also complementary in several aspects:
the former gradually refines an abstraction with a small probability of failure,
while the latter coarsens an abstraction with a small probability of taking longer than expected.
The former parallelizes more easily than $\ActiveCoarsen$,
but also has a worse dependence on $s$ (an upper bound on the size of minimal abstractions).
Therefore, $\StatRefine$ excels for queries requiring small $s$,
but $\ActiveCoarsen$ is better for queries requiring large $s$.

% Hybrid approach in practice
We found it empirically useful to create a hybrid approach (which we denote as $\Learn$).
First, $\Learn$ calls $\StatRefine(\alpha, \emptyset, \bone)$ but with two modifications:
(i) instead of calling $\bF$ to obtain new examples every iteration, we
use the examples from the first iteration which are still valid (greater than $\Jlb$);
and (ii) we stop refining when the number of positive examples greater than $\Jlb$ is zero.
Next, $\Learn$ calls $\ActiveCoarsen(\alpha, \Jlb, \bone)$, where $\Jlb$ is computed from before.
However, since $\StatRefine$ errs with some probability, we call
$\ScanCoarsen(\emptyset, \Jub)$ in the first line of $\ActiveCoarsen$ to ensure
that we find a minimal abstraction.
Note that $\StatRefine$ is useful, for if it discovers relevant components,
then those will always be activated, thus reducing the effective value of $s$
(the number of additional components needed) for $\ActiveCoarsen$.
